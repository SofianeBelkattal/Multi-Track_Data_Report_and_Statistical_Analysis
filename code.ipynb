{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8fac9-0168-42b2-8d71-d9bc97c8df74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "70fc3ea9-378c-432c-b883-8930a0de1705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:15:40.099988Z",
     "start_time": "2025-11-28T20:15:39.964354Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class StudentDataPipeline:\n",
    "    def __init__(self, window=10):\n",
    "        # We initialize our pipeline and define which columns belong to which category\n",
    "        self.window = window\n",
    "        self.id_columns = [\"student_id\"]\n",
    "        self.numeric_columns = [\"attendance\", \"project_score\", \"subject_score\"]\n",
    "        self.categorical_columns = [\"income_student\", \"track\"]\n",
    "        self.percent_columns = [\"attendance\"]\n",
    "        self.allowed_categories = {\"income_student\": [\"Yes\", \"No\"]}\n",
    "\n",
    "    # ---------------------------\n",
    "    # LOAD & MERGE\n",
    "    # ---------------------------\n",
    "    def load_excel(self, path):\n",
    "        # We detect the correct Excel engine and load all sheets\n",
    "        engine = \"openpyxl\" if path.lower().endswith(\".xlsx\") else \"xlrd\"\n",
    "        sheets = pd.read_excel(path, sheet_name=None, engine=engine, dtype=object)\n",
    "\n",
    "        frames = []\n",
    "        # We iterate over each sheet, attach its sheet name as a track, and collect the dataframes\n",
    "        for name, df in sheets.items():\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            df = df.copy()\n",
    "            df[\"track\"] = name\n",
    "            frames.append(df)\n",
    "\n",
    "        # We merge all sheets into a single dataframe\n",
    "        if not frames:\n",
    "            return pd.DataFrame()\n",
    "        merged = pd.concat(frames, ignore_index=True, sort=False)\n",
    "        return merged\n",
    "\n",
    "    # ---------------------------\n",
    "    # COLUMN NORMALIZATION\n",
    "    # ---------------------------\n",
    "    def normalize_columns(self, df):\n",
    "        # We standardize column names and rename known variants to a unified schema\n",
    "        df = df.copy()\n",
    "        df.columns = [c.strip().lower().replace(\" \", \"_\") if isinstance(c, str) else c for c in df.columns]\n",
    "\n",
    "        rename_map = {\n",
    "            \"student_id\": \"student_id\",\n",
    "            \"attendance_%\": \"attendance\",\n",
    "            \"attendance\": \"attendance\",\n",
    "            \"project\": \"project_score\",\n",
    "            \"project_score\": \"project_score\",\n",
    "            \"subject\": \"subject_score\",\n",
    "            \"subject_score\": \"subject_score\",\n",
    "            \"income_student\": \"income_student\",\n",
    "            \"helped\": \"helped\",\n",
    "            \"helped?\": \"helped\",\n",
    "            \"income\": \"income\",\n",
    "        }\n",
    "        df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "        # We remove columns that should not be kept\n",
    "        for col in (\"helped\", \"income\"):\n",
    "            if col in df.columns:\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "        return df\n",
    "\n",
    "    # ---------------------------\n",
    "    # CLEANING & VALUE NORMALIZATION\n",
    "    # ---------------------------\n",
    "    def normalize_values(self, df):\n",
    "        # We trim whitespace and prepare string values\n",
    "        df = df.copy()\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        # We replace known missing-value tokens with NaN\n",
    "        missing_tokens = {\"waive\", \"waived\", \"waiver\", \"n/a\", \"na\", \"none\", \"null\", \"-\", \"--\", \"\"}\n",
    "        for c in df.columns:\n",
    "            df[c] = df[c].apply(lambda x: np.nan if isinstance(x, str) and x.strip().lower() in missing_tokens else x)\n",
    "\n",
    "        # We normalize categorical values for income_student\n",
    "        if \"income_student\" in df.columns:\n",
    "            s = df[\"income_student\"].astype(\"string\").str.strip().str.lower()\n",
    "            mapping = {\n",
    "                \"yes\": \"Yes\", \"y\": \"Yes\", \"true\": \"Yes\", \"t\": \"Yes\", \"1\": \"Yes\",\n",
    "                \"no\": \"No\", \"n\": \"No\", \"false\": \"No\", \"f\": \"No\", \"0\": \"No\",\n",
    "            }\n",
    "            df[\"income_student\"] = s.map(mapping)\n",
    "            df[\"income_student\"] = df[\"income_student\"].where(df[\"income_student\"].notna(), np.nan)\n",
    "        return df\n",
    "\n",
    "    # ---------------------------\n",
    "    # TYPE CONVERSIONS\n",
    "    # ---------------------------\n",
    "    def convert_dtypes(self, df):\n",
    "        # We convert percentage-like columns into numeric percentages\n",
    "        df = df.copy()\n",
    "        for col in self.percent_columns:\n",
    "            if col in df.columns:\n",
    "                s = df[col].astype(str)\n",
    "                has_pct = s.str.contains(\"%\", na=False)\n",
    "                s = s.str.replace(\"%\", \"\", regex=False)\n",
    "                s = pd.to_numeric(s, errors=\"coerce\")\n",
    "                s = pd.Series(np.where(has_pct, s, np.where(s <= 1, s * 100, s)), index=df.index)\n",
    "                df[col] = s\n",
    "\n",
    "        # We convert numeric columns to floats\n",
    "        for col in self.numeric_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # We ensure income_student stays as string\n",
    "        if \"income_student\" in df.columns:\n",
    "            df[\"income_student\"] = df[\"income_student\"].astype(\"string\")\n",
    "        return df\n",
    "\n",
    "    # ---------------------------\n",
    "    # MISSING VALUE HANDLING\n",
    "    # ---------------------------\n",
    "    def impute_missing(self, df):\n",
    "        # We fill missing numeric and categorical values using rolling statistics\n",
    "        df = df.copy()\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # We apply rolling mean imputation for numeric columns\n",
    "        for col in self.numeric_columns:\n",
    "            if col in df.columns:\n",
    "                stats = df[col].shift(1).rolling(window=self.window, min_periods=1).mean()\n",
    "                df[col] = df[col].fillna(stats)\n",
    "                if df[col].isna().any():\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "        # We apply rolling mode imputation for categorical income_student\n",
    "        if \"income_student\" in df.columns:\n",
    "            rolled_mode = (\n",
    "                df[\"income_student\"].shift(1)\n",
    "                .rolling(window=self.window, min_periods=1)\n",
    "                .apply(lambda x: pd.Series(x).mode().iloc[0] if not pd.Series(x).mode().empty else np.nan)\n",
    "            )\n",
    "            df[\"income_student\"] = df[\"income_student\"].fillna(rolled_mode)\n",
    "            if df[\"income_student\"].isna().any():\n",
    "                mode = df[\"income_student\"].mode(dropna=True)\n",
    "                if not mode.empty:\n",
    "                    df[\"income_student\"] = df[\"income_student\"].fillna(mode.iloc[0])\n",
    "        return df\n",
    "\n",
    "    # ---------------------------\n",
    "    # SUMMARY ROW (APPEND AT END)\n",
    "    # ---------------------------\n",
    "    def append_overall_summary_row(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # We compute summary statistics and append a synthetic row at the end\n",
    "        num_cols = [c for c in [\"attendance\", \"subject_score\", \"project_score\"] if c in df.columns]\n",
    "        n_students = df[\"student_id\"].value_counts().count() if \"student_id\" in df.columns else len(df)\n",
    "\n",
    "        if \"student_count\" not in df.columns:\n",
    "            df = df.copy()\n",
    "            df[\"student_count\"] = np.nan\n",
    "\n",
    "        row = {c: np.nan for c in df.columns}\n",
    "        if \"track\" in df.columns:\n",
    "            row[\"track\"] = \"__OVERALL__\"\n",
    "        for c in num_cols:\n",
    "            row[c] = df[c].mean(skipna=True)\n",
    "        row[\"student_count\"] = n_students\n",
    "\n",
    "        return pd.concat([df, pd.DataFrame([row], columns=df.columns)], ignore_index=True)\n",
    "\n",
    "    # ---------------------------\n",
    "    # VALIDATION & EXPORT\n",
    "    # ---------------------------\n",
    "    def validate(self, df):\n",
    "        # We check for invalid ranges and unexpected categorical values\n",
    "        report = {}\n",
    "        for col in [\"attendance\", \"project_score\", \"subject_score\"]:\n",
    "            if col in df.columns:\n",
    "                invalid = df[(df[col].notna()) & ((df[col] < 0) | (df[col] > 100))]\n",
    "                report[f\"{col}_out_of_range\"] = len(invalid)\n",
    "\n",
    "        if \"income_student\" in df.columns:\n",
    "            bad = df[\"income_student\"].dropna().astype(\"string\")\n",
    "            report[\"income_student_invalid\"] = int((~bad.isin(self.allowed_categories[\"income_student\"])).sum())\n",
    "\n",
    "        report[\"remaining_nulls_per_column\"] = {\n",
    "            col: int(df[col].isna().sum()) for col in df.columns if df[col].isna().any()\n",
    "        }\n",
    "        return report\n",
    "\n",
    "    def export_csv(self, df, path):\n",
    "        # We export the cleaned dataframe to CSV\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "    # ---------------------------\n",
    "    # FULL PIPELINE RUNNER\n",
    "    # ---------------------------\n",
    "    def run(self, excel_path, output_csv):\n",
    "        # We orchestrate all processing steps of the pipeline\n",
    "        df = self.load_excel(excel_path)\n",
    "        df = self.normalize_columns(df)\n",
    "        df = self.normalize_values(df)\n",
    "        df = self.convert_dtypes(df)\n",
    "        df = self.impute_missing(df)\n",
    "\n",
    "        report = self.validate(df)\n",
    "        self.export_csv(df, output_csv)\n",
    "\n",
    "        # We print results to confirm completion\n",
    "        print(\"Pipeline complete.\")\n",
    "        print(\"Validation report:\", report)\n",
    "        print(f\"Exported to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = StudentDataPipeline(window=10)\n",
    "    pipeline.run(\"students.xlsx\", \"merged.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline complete.\n",
      "Validation report: {'remaining_nulls_per_column': {'studentid': 1, 'math': 9, 'english': 6, 'science': 6, 'history': 4, 'attendance_(%)': 13, 'incomestudent': 1}}\n",
      "Exported to merged.csv\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "a4c41a8c-2477-4d13-9487-e5a5daad4481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:15:52.348341Z",
     "start_time": "2025-11-28T20:15:52.273359Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# UNIVERSAL COLUMN FINDER (ROBUST FOR ANY DATASET)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def find_column(df, keywords):\n",
    "    \"\"\"\n",
    "    We try to find a column whose name contains any of the given keywords.\n",
    "    Consequently, we normalize column names to make matching easier.\n",
    "    \"\"\"\n",
    "    normalized = {\n",
    "        col: col.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col in df.columns\n",
    "    }\n",
    "\n",
    "    # Here, we iterate through each keyword and, subsequently, each normalized column\n",
    "    for key in keywords:\n",
    "        key_norm = key.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col, norm in normalized.items():\n",
    "            # If the keyword appears in the column name, we immediately return it\n",
    "            if key_norm in norm:\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# DETECT METRIC COLUMNS (math, english, history, project…)\n",
    "# -------------------------------------------------------\n",
    "def get_metric_numeric_columns(df):\n",
    "    # Firstly, we collect all numeric columns from the dataframe\n",
    "    all_numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Then, we define keywords that should help us identify academic scores\n",
    "    keep_keywords = [\n",
    "        \"attend\", \"score\", \"grade\", \"note\", \"math\", \"english\",\n",
    "        \"science\", \"history\", \"project\", \"exam\", \"test\", \"mark\"\n",
    "    ]\n",
    "    drop_keywords = [\"id\", \"code\", \"phone\", \"num\", \"year\", \"age\"]\n",
    "\n",
    "    metrics = []\n",
    "    # We check each numeric column and, therefore, decide whether it is an academic metric\n",
    "    for col in all_numeric:\n",
    "        name = col.lower()\n",
    "        if any(k in name for k in keep_keywords) and not any(b in name for b in drop_keywords):\n",
    "            metrics.append(col)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# PASS RATE HELPER\n",
    "# -------------------------------------------------------\n",
    "def compute_pass_rate(series):\n",
    "    # Here, we convert each value to a lowercase string so that comparisons become consistent\n",
    "    s = series.astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Next, we define words that represent \"yes\" or \"no\" in various forms\n",
    "    yes = {\"y\", \"yes\", \"1\", \"true\", \"t\", \"pass\", \"passed\"}\n",
    "    no = {\"n\", \"no\", \"0\", \"false\", \"f\", \"fail\", \"failed\"}\n",
    "\n",
    "    # We normalize the pass column into Yes/No values, accordingly\n",
    "    normalized = s.apply(lambda x: \"Yes\" if x in yes else (\"No\" if x in no else np.nan))\n",
    "\n",
    "    # Finally, we compute the percentage of \"Yes\" values\n",
    "    return (normalized == \"Yes\").mean() * 100\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# MAIN STATS REPORT (TRACK + COHORT)\n",
    "# -------------------------------------------------------\n",
    "def compute_track_stats(cleaned_csv, stats_output_excel):\n",
    "    # Initially, we load the cleaned dataset\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # AUTO-DETECT IMPORTANT COLUMNS\n",
    "    # ---------------------------------------------------\n",
    "    # Here, we attempt to automatically determine essential column names\n",
    "    track_col = find_column(df, [\"track\"])\n",
    "    cohort_col = find_column(df, [\"cohort\", \"cohorte\", \"promo\", \"generation\", \"year\"])\n",
    "    pass_col = find_column(df, [\"pass\", \"passed\", \"result\", \"status\"])\n",
    "    attendance_col = find_column(df, [\"attendance\", \"attend\", \"presence\"])\n",
    "    project_col = find_column(df, [\"project\", \"projectscore\", \"proj\"])\n",
    "\n",
    "    if track_col is None:\n",
    "        raise ValueError(\"ERROR: Track column not found.\")\n",
    "\n",
    "    # We normalize column names by renaming them once detected\n",
    "    df.rename(columns={track_col: \"track\"}, inplace=True)\n",
    "    if cohort_col: df.rename(columns={cohort_col: \"cohort\"}, inplace=True)\n",
    "    if pass_col: df.rename(columns={pass_col: \"pass_col\"}, inplace=True)\n",
    "    if attendance_col: df.rename(columns={attendance_col: \"attendance\"}, inplace=True)\n",
    "    if project_col: df.rename(columns={project_col: \"projectscore\"}, inplace=True)\n",
    "\n",
    "    # We convert attendance and projectscore to numeric, because calculations require numbers\n",
    "    if \"attendance\" in df.columns:\n",
    "        df[\"attendance\"] = pd.to_numeric(df[\"attendance\"], errors=\"coerce\")\n",
    "    if \"projectscore\" in df.columns:\n",
    "        df[\"projectscore\"] = pd.to_numeric(df[\"projectscore\"], errors=\"coerce\")\n",
    "\n",
    "    # Now, we detect all academic metric columns dynamically\n",
    "    metric_cols = get_metric_numeric_columns(df)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # TRACK-LEVEL STATS\n",
    "    # ---------------------------------------------------\n",
    "    # We compute the number of students per track\n",
    "    track_count = df.groupby(\"track\").size().rename(\"student_count\")\n",
    "\n",
    "    # Next, we compute the mean of each metric by track\n",
    "    track_mean = df.groupby(\"track\")[metric_cols].mean()\n",
    "\n",
    "    # Additionally, we compute the pass rate per track when possible\n",
    "    if \"pass_col\" in df.columns:\n",
    "        track_pass = df.groupby(\"track\")[\"pass_col\"].apply(compute_pass_rate).rename(\"pass_rate\")\n",
    "    else:\n",
    "        track_pass = pd.Series(np.nan, index=track_count.index, name=\"pass_rate\")\n",
    "\n",
    "    # Then, we compute the correlation between attendance and project score for each track\n",
    "    corr_series = pd.Series(index=track_count.index, dtype=float, name=\"corr_attendance_project\")\n",
    "    if \"attendance\" in df.columns and \"projectscore\" in df.columns:\n",
    "        for t, g in df.groupby(\"track\"):\n",
    "            corr_series.loc[t] = g[\"attendance\"].corr(g[\"projectscore\"])\n",
    "    else:\n",
    "        corr_series[:] = np.nan\n",
    "\n",
    "    # Consequently, we combine all computed metrics into a single dataframe\n",
    "    track_stats = pd.concat([track_count, track_mean, track_pass, corr_series], axis=1).reset_index()\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # ADD OVERALL ROW TO TRACK SHEET\n",
    "    # ---------------------------------------------------\n",
    "    # We add a summary row that synthesizes overall statistics\n",
    "    overall_track = {\"track\": \"__OVERALL__\", \"student_count\": track_count.sum()}\n",
    "\n",
    "    for col in track_stats.columns:\n",
    "        if col not in [\"track\", \"student_count\"]:\n",
    "            overall_track[col] = \"\"\n",
    "\n",
    "    track_stats = pd.concat([track_stats, pd.DataFrame([overall_track])], ignore_index=True)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # COHORT-LEVEL STATS (WITH OVERALL)\n",
    "    # ---------------------------------------------------\n",
    "    if \"cohort\" in df.columns:\n",
    "        # As before, we compute cohort-level counts and metrics\n",
    "        cohort_count = df.groupby(\"cohort\").size().rename(\"student_count\")\n",
    "        cohort_mean = df.groupby(\"cohort\")[metric_cols].mean()\n",
    "\n",
    "        if \"pass_col\" in df.columns:\n",
    "            cohort_pass = df.groupby(\"cohort\")[\"pass_col\"].apply(compute_pass_rate).rename(\"pass_rate\")\n",
    "        else:\n",
    "            cohort_pass = pd.Series(np.nan, index=cohort_count.index, name=\"pass_rate\")\n",
    "\n",
    "        cohort_stats = pd.concat([cohort_count, cohort_mean, cohort_pass], axis=1).reset_index()\n",
    "\n",
    "        # Then, we add an overall summary row for cohorts\n",
    "        overall_cohort = {\"cohort\": \"__OVERALL__\", \"student_count\": cohort_count.sum()}\n",
    "\n",
    "        for col in cohort_stats.columns:\n",
    "            if col not in [\"cohort\", \"student_count\"]:\n",
    "                overall_cohort[col] = \"\"\n",
    "\n",
    "        cohort_stats = pd.concat([cohort_stats, pd.DataFrame([overall_cohort])], ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        # If no cohort column exists, we clearly indicate it\n",
    "        cohort_stats = pd.DataFrame({\"cohort\": [\"No cohort column found\"], \"student_count\": [\"\"]})\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # SAVE EXCEL WITH TWO SHEETS\n",
    "    # ---------------------------------------------------\n",
    "    # Finally, we save our results in an Excel file with two separate sheets\n",
    "    with pd.ExcelWriter(stats_output_excel, engine=\"openpyxl\") as writer:\n",
    "        track_stats.to_excel(writer, sheet_name=\"By_Track\", index=False)\n",
    "        cohort_stats.to_excel(writer, sheet_name=\"By_Cohort\", index=False)\n",
    "\n",
    "    # We also print previews to help us quickly verify the results\n",
    "    print(\"\\n=== Stats Report Generated ===\")\n",
    "    print(\"Saved to:\", stats_output_excel)\n",
    "    print(\"\\nTrack sheet preview:\")\n",
    "    print(track_stats.tail())\n",
    "    print(\"\\nCohort sheet preview:\")\n",
    "    print(cohort_stats.tail())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # We simply call the main function when running the script\n",
    "    compute_track_stats(\"merged.csv\", \"stats_report.xlsx\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stats Report Generated ===\n",
      "Saved to: stats_report.xlsx\n",
      "\n",
      "Track sheet preview:\n",
      "         track  student_count       math    english    science    history  \\\n",
      "0           BM            200   64.45736  64.968342  65.989447  64.824121   \n",
      "1         Data            200  72.308081  71.323232  72.532995  72.737374   \n",
      "2      Finance            200  62.434694  64.948731  66.466667  63.536683   \n",
      "3  __OVERALL__            600                                               \n",
      "\n",
      "  attendance projectscore pass_rate corr_attendance_project  \n",
      "0  78.997462       71.398      91.5                -0.05283  \n",
      "1  84.137436         72.2      90.0               -0.068687  \n",
      "2  79.963077      69.2345      95.0               -0.047074  \n",
      "3                                                            \n",
      "\n",
      "Cohort sheet preview:\n",
      "        cohort  student_count       math    english    science    history  \\\n",
      "0        25-26            198  66.655897  68.252792  69.618653  67.538071   \n",
      "1        26-27            194  67.153684  65.919792  66.840933  65.362694   \n",
      "2        27-28            208   65.51068      67.04  68.483654  68.086408   \n",
      "3  __OVERALL__            600                                               \n",
      "\n",
      "  attendance projectscore  pass_rate  \n",
      "0  81.158549    71.447475  92.929293  \n",
      "1  80.095213    70.899485  91.237113  \n",
      "2  81.750485    70.506731  92.307692  \n",
      "3                                     \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "f7299652-a5ec-406c-bf1b-217cb89da832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:37:54.834583Z",
     "start_time": "2025-11-28T20:37:54.820149Z"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------\n",
    "# UNIVERSAL COLUMN FINDER (reuse this everywhere)\n",
    "# ----------------------------------------------\n",
    "def find_column(df, keywords):\n",
    "    # We first normalize all column names so that matching becomes easier and, consequently, more reliable\n",
    "    normalized = {\n",
    "        col: col.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col in df.columns\n",
    "    }\n",
    "    # Then, we iterate through each keyword and compare with each normalized column name\n",
    "    for key in keywords:\n",
    "        key_norm = key.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col, norm in normalized.items():\n",
    "            # If a keyword is contained in a column name, we immediately return it\n",
    "            if key_norm in norm:\n",
    "                return col\n",
    "    # If nothing matches, we return None because we want to avoid false detections\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SAFE HISTORY DISTRIBUTION PLOTS (per track)\n",
    "# -------------------------------------------------------\n",
    "def plot_history_per_track(cleaned_csv):\n",
    "    # First, we load the cleaned CSV so we can start analyzing the data\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "\n",
    "    # 1) Find the history column safely\n",
    "    # Here, we attempt to locate any column related to history by checking several keyword variants\n",
    "    history_col = find_column(df, [\"history\", \"hist\", \"histoire\", \"socialscience\"])\n",
    "    if history_col is None:\n",
    "        # If no such column is found, we stop the process because we cannot plot what does not exist\n",
    "        raise ValueError(\"Could not find any history-like column in dataset.\")\n",
    "\n",
    "    # 2) Convert to numeric (fix WAIVE, strings, etc.)\n",
    "    # We then attempt to convert the identified column to numeric, since history scores must be numbers\n",
    "    df[history_col] = pd.to_numeric(df[history_col], errors=\"coerce\")\n",
    "\n",
    "    # We remove rows where the history score is missing because they cannot be plotted meaningfully\n",
    "    df = df.dropna(subset=[history_col])\n",
    "\n",
    "    # 3) Find track column safely\n",
    "    # Next, we locate the track column so we can produce one histogram per track\n",
    "    track_col = find_column(df, [\"track\"])\n",
    "    if track_col is None:\n",
    "        raise ValueError(\"Track column missing.\")\n",
    "\n",
    "    # 4) Plot one histogram per track\n",
    "    # We extract the list of unique track names so we can iterate through them\n",
    "    tracks = df[track_col].unique()\n",
    "\n",
    "    for t in tracks:\n",
    "        # For each track, we isolate the subset of students belonging to it\n",
    "        subset = df[df[track_col] == t]\n",
    "\n",
    "        # If the subset is empty (which rarely happens), we skip it entirely\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        # We create a large figure so the histogram looks clear\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Here, we plot the distribution using seaborn's histplot, adding a KDE to better visualize the shape\n",
    "        sns.histplot(\n",
    "            subset[history_col],\n",
    "            bins=20,\n",
    "            kde=True,\n",
    "            color=\"darkorchid\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        # We add a title and axis labels to ensure the plot is understandable\n",
    "        plt.title(f\"Distribution of {history_col} — {t} Track\")\n",
    "        plt.xlabel(history_col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # We save the plot so we can inspect it later\n",
    "        plt.savefig(f\"{t}_history.png\")\n",
    "\n",
    "        # We close the figure to avoid excessive memory usage\n",
    "        plt.close()\n",
    "\n",
    "        # We show the plot directly, in case we're running this interactively\n",
    "        plt.show()\n",
    "\n",
    "    # Finally we print which column was identified as history to give the user quick feedback\n",
    "    print(\"History column detected:\", history_col)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "ba5b39b6-ce2c-4c43-b122-75390ccbee82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:16:10.780612Z",
     "start_time": "2025-11-28T20:16:10.050841Z"
    }
   },
   "source": [
    "plot_history_per_track(\"merged.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History column detected: history\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "8ed9ce34-81b4-410f-8762-1e785153e90c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:16:16.495075Z",
     "start_time": "2025-11-28T20:16:16.476962Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# UNIVERSAL COLUMN FINDER (reuse this)\n",
    "# -------------------------------------------------------\n",
    "# We add detailed comments written as students reflecting on our reasoning.\n",
    "def find_column(df, keywords):\n",
    "    # Here we begin by normalizing every column name so that comparisons become simpler and, consequently, more reliable\n",
    "    normalized = {\n",
    "        col: col.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col in df.columns\n",
    "    }\n",
    "\n",
    "    # Next, we iterate through each keyword, because we want to check whether any of them appears in a column name\n",
    "    for key in keywords:\n",
    "        key_norm = key.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col, norm in normalized.items():\n",
    "            # If the keyword is included in a column, we return that column immediately\n",
    "            if key_norm in norm:\n",
    "                return col\n",
    "    # If nothing matches, we logically return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Detect academic metric numerical columns\n",
    "# -------------------------------------------------------\n",
    "def get_metric_numeric_columns(df):\n",
    "    # First, we extract all numeric columns because only these can represent academic scores\n",
    "    all_numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Then, we define keywords that usually indicate academic performance\n",
    "    keep_keywords = [\n",
    "        \"attend\", \"score\", \"grade\", \"note\", \"math\", \"english\",\n",
    "        \"science\", \"history\", \"project\", \"exam\", \"test\", \"mark\"\n",
    "    ]\n",
    "\n",
    "    # Conversely, we exclude numeric columns that clearly do not represent academic scores\n",
    "    drop_keywords = [\"id\", \"num\", \"code\", \"year\", \"age\"]\n",
    "\n",
    "    metrics = []\n",
    "    # We evaluate every numeric column to determine whether it should be kept\n",
    "    for col in all_numeric:\n",
    "        name = col.lower()\n",
    "        if any(k in name for k in keep_keywords) and not any(b in name for b in drop_keywords):\n",
    "            metrics.append(col)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Income vs Non-Income Comparison (Grouped Bar Plot)\n",
    "# -------------------------------------------------------\n",
    "def plot_income_comparison(cleaned_csv):\n",
    "    # We load the cleaned data so that we can perform our comparison\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "\n",
    "    # 1) Detect the income column automatically\n",
    "    # Here, we try to locate any column related to financial support or scholarships\n",
    "    income_col = find_column(df, [\"income\", \"support\", \"aid\", \"help\", \"bourse\", \"scholar\"])\n",
    "    if income_col is None:\n",
    "        raise ValueError(\"Could not find income/student-support column in dataset.\")\n",
    "\n",
    "    # We convert all values into a consistent Yes/No format so our comparisons are meaningful\n",
    "    df[income_col] = df[income_col].astype(str).str.lower().str.strip()\n",
    "    df[income_col] = df[income_col].map({\n",
    "        \"yes\": \"Yes\", \"y\": \"Yes\", \"1\": \"Yes\", \"true\": \"Yes\", \"vrai\": \"Yes\",\n",
    "        \"no\": \"No\", \"n\": \"No\", \"0\": \"No\", \"false\": \"No\", \"faux\": \"No\"\n",
    "    })\n",
    "\n",
    "    # 2) Identify academic metric columns\n",
    "    # Now we determine which numeric columns represent academic results\n",
    "    metric_cols = get_metric_numeric_columns(df)\n",
    "\n",
    "    # Then, we make sure these values are numeric so the averages we compute later are correct\n",
    "    for col in metric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # 3) Compute the average per income group\n",
    "    # Here we group students by whether they receive financial support and calculate the mean of each metric\n",
    "    summary = df.groupby(income_col)[metric_cols].mean()\n",
    "\n",
    "    # We transpose so that subjects appear on rows, which consequently makes the bar plot more readable\n",
    "    summary = summary.T\n",
    "\n",
    "    # 4) Plot grouped bar chart\n",
    "    # We create a bar chart comparing performance between students with and without income support\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    summary.plot(kind=\"bar\", color=[\"darkorchid\", \"indigo\"], figsize=(12, 6))\n",
    "\n",
    "    # We label our plot carefully so viewers immediately understand what is being shown\n",
    "    plt.title(\"Academic Performance — Income Support vs Non-Support\")\n",
    "    plt.xlabel(\"Subjects\")\n",
    "    plt.ylabel(\"Average Score\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.legend(title=\"Income Support\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # We save the figure so we can use it later\n",
    "    plt.savefig(\"income_support_vs_non.png\")\n",
    "\n",
    "    # We close and then show the plot to avoid clutter\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "\n",
    "    # Finally, we print a quick summary so we can verify the correct columns were detected\n",
    "    print(\"Income column detected:\", income_col)\n",
    "    print(\"Metrics used:\", metric_cols)\n",
    "    print(\"Summary:\")\n",
    "    print(summary)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "acbf68a0-2e55-42e6-9db2-6332cb34c1ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:16:23.262124Z",
     "start_time": "2025-11-28T20:16:23.068550Z"
    }
   },
   "source": [
    "plot_income_comparison(\"merged.csv\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income column detected: incomestudent\n",
      "Metrics used: ['math', 'english', 'science', 'history', 'attendance_(%)', 'projectscore']\n",
      "Summary:\n",
      "incomestudent          No        Yes\n",
      "math            67.678386  64.477778\n",
      "english         67.474857  66.600412\n",
      "science         68.796275  67.673361\n",
      "history         67.461254  66.386066\n",
      "attendance_(%)  81.622899  80.201245\n",
      "projectscore    70.918697  71.028862\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "c3c61a6a-59c5-4b66-b13a-0f955feb0da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:16:38.138731Z",
     "start_time": "2025-11-28T20:16:38.126319Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------\n",
    "# UNIVERSAL COLUMN FINDER\n",
    "# ----------------------------------------------\n",
    "# We write comments as students thinking through each step, using explicit reasoning and linking words.\n",
    "def find_column(df, keywords):\n",
    "    # First, we normalize all column names so that, consequently, comparisons become easier\n",
    "    normalized = {\n",
    "        col: col.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col in df.columns\n",
    "    }\n",
    "    # Next, we iterate through every keyword and compare with the normalized names\n",
    "    for key in keywords:\n",
    "        key_norm = key.lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "        for col, norm in normalized.items():\n",
    "            # If the keyword is found inside a column name, we immediately select it\n",
    "            if key_norm in norm:\n",
    "                return col\n",
    "    # If we reach this point, then no matching column was found\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# COMPARAISON DES MOYENNES DE MATH PAR TRACK\n",
    "# -------------------------------------------------------\n",
    "def plot_math_by_track(cleaned_csv):\n",
    "    # To begin with, we load the cleaned CSV file into a DataFrame\n",
    "    df = pd.read_csv(cleaned_csv)\n",
    "\n",
    "    # 1) Trouver la colonne Track automatiquement\n",
    "    # Here, we try to identify which column corresponds to the track/class/section\n",
    "    track_col = find_column(df, [\"track\", \"classe\", \"section\"])\n",
    "    if track_col is None:\n",
    "        raise ValueError(\"Could not find track column in dataset.\")\n",
    "\n",
    "    # 2) Trouver la colonne Math automatiquement\n",
    "    # Likewise, we attempt to locate any column that likely represents math results\n",
    "    math_col = find_column(df, [\"math\", \"mathematique\", \"note_math\", \"note\", \"mat\"])\n",
    "    if math_col is None:\n",
    "        raise ValueError(\"Could not find math column in dataset.\")\n",
    "\n",
    "    # 3) Convertir en numérique (safe)\n",
    "    # Since math grades must be numeric for averaging, we safely convert them\n",
    "    df[math_col] = pd.to_numeric(df[math_col], errors=\"coerce\")\n",
    "\n",
    "    # 4) Calculer les moyennes par track\n",
    "    # Then, we compute the average math score per track; consequently, we obtain a clear comparison\n",
    "    summary = df.groupby(track_col)[math_col].mean().sort_values(ascending=False)\n",
    "\n",
    "    # 5) Plot\n",
    "    # Now we create a bar plot to visualize differences in average math performance across tracks\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=summary.index,\n",
    "        y=summary.values,\n",
    "        palette=\"Purples\",\n",
    "    )\n",
    "\n",
    "    # We add a title and axis labels so that the plot remains easy to interpret\n",
    "    plt.title(\"Average Math Score per Track\", fontsize=14)\n",
    "    plt.xlabel(\"Track\")\n",
    "    plt.ylabel(\"Average Math Score\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    # We save the figure in case we need to include it in a report later\n",
    "    plt.savefig(\"math_by_track.png\")\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "\n",
    "    # Finally, we print the detected columns and the summary table to verify everything makes sense\n",
    "    print(\"Track column detected :\", track_col)\n",
    "    print(\"Math column detected :\", math_col)\n",
    "    print(\"\\nSummary (math mean per track):\")\n",
    "    print(summary)\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "5a8c443e-0fec-4ae6-8e4f-ff0b53739427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T20:16:45.949056Z",
     "start_time": "2025-11-28T20:16:45.769180Z"
    }
   },
   "source": [
    "plot_math_by_track(\"merged.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track column detected : track\n",
      "Math column detected : math\n",
      "\n",
      "Summary (math mean per track):\n",
      "track\n",
      "Data       72.308081\n",
      "BM         64.457360\n",
      "Finance    62.434694\n",
      "Name: math, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_3156\\1112421470.py:57: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
